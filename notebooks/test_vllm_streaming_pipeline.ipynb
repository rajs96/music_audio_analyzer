{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test vLLM Streaming Pipeline (Single GPU)\n",
    "\n",
    "Simple test to verify the streaming pipeline works with vLLM on a single GPU.\n",
    "\n",
    "**Key steps:**\n",
    "1. Download model files BEFORE starting Ray\n",
    "2. Initialize Ray with proper runtime_env\n",
    "3. Run the streaming pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import hashlib\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-Omni-30B-A3B-Instruct\"\n",
    "MODELS_DIR = \"/app/models\"\n",
    "NUM_TEST_FILES = 5\n",
    "WARMUP_TIMEOUT = 300.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Download Model Files (BEFORE Ray)\n",
    "\n",
    "This downloads all model files to a local directory without loading into GPU memory.\n",
    "vLLM will then load from this local path, which is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model_for_vllm(model_name: str, models_dir: str) -> str:\n",
    "    \"\"\"Download model files for vLLM using huggingface_hub snapshot_download.\"\"\"\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    model_cache_name = model_name.replace(\"/\", \"_\")\n",
    "    local_path = Path(models_dir) / model_cache_name\n",
    "\n",
    "    if local_path.exists() and any(local_path.iterdir()):\n",
    "        print(f\"Model already downloaded at {local_path}\")\n",
    "        return str(local_path)\n",
    "\n",
    "    print(f\"Downloading model {model_name} to {local_path}...\")\n",
    "    print(\"This may take a while on first run...\")\n",
    "    local_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    snapshot_download(\n",
    "        repo_id=model_name,\n",
    "        local_dir=str(local_path),\n",
    "        local_dir_use_symlinks=False,\n",
    "    )\n",
    "\n",
    "    print(f\"Model downloaded successfully to {local_path}\")\n",
    "    return str(local_path)\n",
    "\n",
    "\n",
    "# Download model BEFORE starting Ray\n",
    "MODEL_PATH = download_model_for_vllm(MODEL_NAME, MODELS_DIR)\n",
    "print(f\"Using model from: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Ray with runtime_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.util.queue import Queue\n",
    "\n",
    "# Shutdown any existing Ray instance\n",
    "ray.shutdown()\n",
    "\n",
    "# Initialize Ray with runtime_env so workers can find 'src'\n",
    "ray.init(\n",
    "    ignore_reinit_error=True,\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\"PYTHONPATH\": \"/app\"},\n",
    "    },\n",
    ")\n",
    "print(f\"Ray initialized: {ray.cluster_resources()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pipeline components\n",
    "from src.streaming_pipeline import (\n",
    "    AgentRayComputeConfig,\n",
    "    AgentStage,\n",
    "    QueueStreamingDatasource,\n",
    "    StreamingDatasourceConfig,\n",
    "    StreamingPipeline,\n",
    ")\n",
    "from src.pipelines.instrument_detection.agents.audio_preprocessor import (\n",
    "    AudioPreprocessorAgent,\n",
    ")\n",
    "from src.pipelines.instrument_detection.agents.instrument_detector import (\n",
    "    InstrumentDetectorAgent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find audio files\n",
    "audio_dir = Path(\"../audio_files\")\n",
    "audio_files = list(audio_dir.glob(\"*.mp3\"))[:NUM_TEST_FILES]\n",
    "print(f\"Found {len(audio_files)} audio files to test:\")\n",
    "for f in audio_files:\n",
    "    print(f\"  - {f.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def create_job_row(filepath: Path) -> dict:\n",
    "    \"\"\"Create a job row from an audio file.\"\"\"\n",
    "    audio_bytes = filepath.read_bytes()\n",
    "    return {\n",
    "        \"job_id\": f\"job_{uuid.uuid4().hex[:8]}\",\n",
    "        \"song_id\": f\"song_{uuid.uuid4().hex[:8]}\",\n",
    "        \"song_hash\": hashlib.sha256(audio_bytes).hexdigest()[:16],\n",
    "        \"filename\": filepath.name,\n",
    "        \"audio_bytes\": audio_bytes,\n",
    "    }\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Pipeline with Local Model Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create job queue and datasource\n",
    "job_queue = Queue(maxsize=100)\n",
    "\n",
    "datasource = QueueStreamingDatasource(\n",
    "    queue=job_queue,\n",
    "    item_to_row_fn=identity,\n",
    "    config=StreamingDatasourceConfig(\n",
    "        parallelism=1,\n",
    "        batch_size=1,\n",
    "        batch_timeout=0.5,\n",
    "        poll_interval=0.1,\n",
    "        max_items=NUM_TEST_FILES,\n",
    "    ),\n",
    ")\n",
    "print(\"Datasource created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline stages\n",
    "\n",
    "# Stage 1: Audio Preprocessor (CPU)\n",
    "preprocessor_stage = AgentStage(\n",
    "    agent=AudioPreprocessorAgent(target_sr=16000),\n",
    "    config=AgentRayComputeConfig(\n",
    "        num_actors=2,\n",
    "        batch_size=1,\n",
    "        num_cpus=1.0,\n",
    "        max_concurrency=1,\n",
    "    ),\n",
    "    name=\"AudioPreprocessor\",\n",
    ")\n",
    "\n",
    "# Stage 2: Instrument Detector with vLLM (GPU)\n",
    "# NOTE: Using MODEL_PATH (local path) instead of MODEL_NAME (HuggingFace repo)\n",
    "detector_stage = AgentStage(\n",
    "    agent=InstrumentDetectorAgent(\n",
    "        model_name=MODEL_PATH,  # Use local path!\n",
    "        use_vllm=True,\n",
    "        tensor_parallel_size=1,\n",
    "        gpu_memory_utilization=0.90,\n",
    "        max_model_len=16384,\n",
    "        max_num_seqs=4,\n",
    "    ),\n",
    "    config=AgentRayComputeConfig(\n",
    "        num_actors=1,\n",
    "        batch_size=1,\n",
    "        num_gpus=1.0,\n",
    "        max_concurrency=1,\n",
    "    ),\n",
    "    name=\"InstrumentDetector\",\n",
    ")\n",
    "\n",
    "print(f\"Stages created (detector using model from: {MODEL_PATH})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "pipeline = StreamingPipeline(\n",
    "    datasource=datasource,\n",
    "    stages=[preprocessor_stage, detector_stage],\n",
    "    name=\"vLLM_InstrumentDetection\",\n",
    ")\n",
    "print(\"Pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup function - uses a real audio file to warm up the pipeline actors\n",
    "# This ensures vLLM loads the model ONCE, then the same actors process all real data\n",
    "def get_warmup_data():\n",
    "    \"\"\"Create warmup data using a real audio file.\"\"\"\n",
    "    warmup_file = audio_files[0]\n",
    "    audio_bytes = warmup_file.read_bytes()\n",
    "    return [\n",
    "        {\n",
    "            \"job_id\": \"warmup_001\",\n",
    "            \"song_id\": \"warmup\",\n",
    "            \"song_hash\": \"warmup\",\n",
    "            \"filename\": warmup_file.name,\n",
    "            \"audio_bytes\": audio_bytes,\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "print(\"Warmup function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Warmup and Stream Results\n\nUsing `warmup_and_stream()` ensures a SINGLE `iter_batches()` call handles both warmup and real data.\nThis is critical for vLLM - the model loads once during warmup, then the same actors process real data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get streaming iterator with warmup\n# Warmup items are injected into the queue FIRST\nprint(\"Starting warmup_and_stream...\")\nprint(\"(vLLM model will load on first batch - may take a few minutes)\")\nprint(\"-\" * 60)\n\nstreaming_iterator = pipeline.warmup_and_stream(\n    warmup_data_fn=get_warmup_data,\n    warmup_timeout=WARMUP_TIMEOUT,\n    batch_size=1,\n)\n\n# NOW submit real jobs - they come AFTER warmup items in the queue\nprint(f\"\\nSubmitting {len(audio_files)} jobs...\")\nfor i, audio_file in enumerate(audio_files):\n    row = create_job_row(audio_file)\n    job_queue.put(row)\n    print(f\"  [{i+1}/{len(audio_files)}] Submitted: {row['filename']}\")\n    time.sleep(0.1)\n\nprint(f\"All {len(audio_files)} jobs submitted\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Stream results - warmup items are automatically discarded by the generator\nresults = []\nstart_time = time.time()\n\nprint(\"\\nStreaming results (warmup handled internally)...\")\nprint(\"-\" * 60)\n\nfor batch in streaming_iterator:\n    if not batch:\n        continue\n\n    keys = list(batch.keys())\n    if not keys:\n        continue\n\n    n_items = len(batch[keys[0]])\n    for i in range(n_items):\n        result = {k: batch[k][i] for k in keys}\n        results.append(result)\n\n        elapsed = time.time() - start_time\n\n        if result.get(\"error\"):\n            print(f\"[{elapsed:.1f}s] {result['filename']} -> ERROR: {result['error']}\")\n        else:\n            instruments = result.get(\"instruments\", [])\n            print(f\"[{elapsed:.1f}s] {result['filename']} -> {instruments}\")\n\n    if len(results) >= NUM_TEST_FILES:\n        print(\"-\" * 60)\n        print(f\"All {NUM_TEST_FILES} results received!\")\n        break\n\ntotal_time = time.time() - start_time\nprint(f\"\\nTotal streaming time: {total_time:.1f}s\")\nprint(f\"Average per file: {total_time / len(results):.1f}s\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "successful = [r for r in results if not r.get(\"error\")]\n",
    "failed = [r for r in results if r.get(\"error\")]\n",
    "\n",
    "print(f\"Total: {len(results)}\")\n",
    "print(f\"Successful: {len(successful)}\")\n",
    "print(f\"Failed: {len(failed)}\")\n",
    "\n",
    "print(\"\\nDetailed results:\")\n",
    "for r in results:\n",
    "    if r.get(\"error\"):\n",
    "        print(f\"  - {r['filename']}: ERROR - {r['error']}\")\n",
    "    else:\n",
    "        print(f\"  - {r['filename']}: {r.get('instruments', [])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "pipeline.stop()\n",
    "ray.shutdown()\n",
    "print(\"Cleanup complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}