{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2fceda1-e0da-4470-89e8-4a624a450e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-31 03:53:42,949\tINFO worker.py:1998 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8268 \u001b[39m\u001b[22m\n",
      "2025-12-31 03:53:42,952\tINFO packaging.py:392 -- Ignoring upload to cluster for these files: [PosixPath('/Users/rajsingh/Desktop/code/music_audio_analyzer/.gitignore')]\n",
      "2025-12-31 03:53:42,985\tINFO packaging.py:691 -- Creating a file package for local module '/Users/rajsingh/Desktop/code/music_audio_analyzer'.\n",
      "2025-12-31 03:53:42,986\tINFO packaging.py:392 -- Ignoring upload to cluster for these files: [PosixPath('/Users/rajsingh/Desktop/code/music_audio_analyzer/.gitignore')]\n",
      "2025-12-31 03:53:43,012\tINFO packaging.py:463 -- Pushing file package 'gcs://_ray_pkg_48ba2e4be7307197.zip' (0.38MiB) to Ray cluster...\n",
      "2025-12-31 03:53:43,013\tINFO packaging.py:476 -- Successfully pushed file package 'gcs://_ray_pkg_48ba2e4be7307197.zip'.\n",
      "\u001b[32m2025-12-31 03:53:45.399\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.streaming_pipeline.streaming_component\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m146\u001b[0m - \u001b[1mRay Data streaming config: preserve_order=False\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 8 audio files\n",
      "[  1.75s] SUBMIT: job_000 (Red Hot Chili Peppers - O...) - 7833KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-12-31 03:53:47.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.streaming_pipeline.streaming_component\u001b[0m:\u001b[36mbuild\u001b[0m:\u001b[36m155\u001b[0m - \u001b[1mAdding stage: MockPreprocessor\u001b[0m\n",
      "2025-12-31 03:53:47,421\tINFO logging.py:397 -- Registered dataset logger for dataset dataset_1_0\n",
      "2025-12-31 03:53:47,428\tINFO streaming_executor.py:178 -- Starting execution of Dataset dataset_1_0. Full logs are in /tmp/ray/session_2025-12-31_03-53-39_968160_9277/logs/ray-data\n",
      "2025-12-31 03:53:47,428\tINFO streaming_executor.py:179 -- Execution plan of Dataset dataset_1_0: InputDataBuffer[Input] -> TaskPoolMapOperator[ReadQueueStreaming] -> ActorPoolMapOperator[MapBatches(AgentCallable)]\n",
      "2025-12-31 03:53:47,450\tINFO streaming_executor.py:686 -- [dataset]: A new progress UI is available. To enable, set `ray.data.DataContext.get_current().enable_rich_progress_bars = True` and `ray.data.DataContext.get_current().use_ray_tqdm = False`.\n",
      "2025-12-31 03:53:47,451\tINFO progress_bar.py:155 -- Progress bar disabled because stdout is a non-interactive terminal.\n",
      "2025-12-31 03:53:47,452\tWARNING resource_manager.py:136 -- ⚠️  Ray's object store is configured to use only 22.6% of available memory (2.0GiB out of 8.9GiB total). For optimal Ray Data performance, we recommend setting the object store to at least 50% of available memory. You can do this by setting the 'object_store_memory' parameter when calling ray.init() or by setting the RAY_DEFAULT_OBJECT_STORE_MEMORY_PROPORTION environment variable.\n",
      "2025-12-31 03:53:47,453\tWARNING default_actor_autoscaler.py:236 -- ⚠️  Actor Pool configuration of the ActorPoolMapOperator[MapBatches(AgentCallable)] will not allow it to scale up: configured utilization threshold (200.0%) couldn't be reached with configured max_concurrency=1 and max_tasks_in_flight_per_actor=1 (max utilization will be max_tasks_in_flight_per_actor / max_concurrency = 100%)\n",
      "2025-12-31 03:53:47,567\tINFO progress_bar.py:213 -- === Ray Data Progress {ReadQueueStreaming->SplitBlocks(200)} ===\n",
      "2025-12-31 03:53:47,568\tINFO progress_bar.py:215 -- ReadQueueStreaming->SplitBlocks(200): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 3.0MiB object store: Progress Completed 0 / ?\n",
      "2025-12-31 03:53:47,569\tINFO progress_bar.py:213 -- === Ray Data Progress {MapBatches(AgentCallable)} ===\n",
      "2025-12-31 03:53:47,569\tINFO progress_bar.py:215 -- MapBatches(AgentCallable): Tasks: 0; Actors: 2 (running=0, restarting=0, pending=2); Queued blocks: 0 (0.0B); Resources: 0.0 CPU, 0.0B object store; [all objects local]: Progress Completed 0 / ?\n",
      "2025-12-31 03:53:47,570\tINFO progress_bar.py:213 -- === Ray Data Progress {Running Dataset} ===\n",
      "2025-12-31 03:53:47,570\tINFO progress_bar.py:215 -- Running Dataset: dataset_1_0. Active & requested resources: 0/10 CPU, 0.0B/1.0GiB object store (pending: 2 CPU): Progress Completed 0 / ?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.77s] SUBMIT: job_001 (Drake - Drew A Picasso.mp...) - 7747KB\n",
      "[  3.79s] SUBMIT: job_002 (Chris Brown, Drake - No G...) - 8256KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:49.575 | INFO     | src.streaming_pipeline.agent:__init__:301 - AgentCallable initialized: FunctionAgent\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:49.629 | INFO     | src.streaming_pipeline.streaming_datasource:make_block_generator:247 - Generator started. queue=<ray.util.queue.Queue object at 0x1162fc5e0>, max_items=8\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:49.638 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:281 - Got item #0\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:49.649 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:281 - Got item #1\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:49.655 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 2 rows, total_read=2\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:49.674 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:281 - Got item #2\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:49.883 | INFO     | __main__:mock_preprocess:48 - Processed Red Hot Chili Peppers - Others... (8021583 bytes)\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:50.086 | INFO     | __main__:mock_preprocess:48 - Processed Drake - Drew A Picasso.mp3... (7933580 bytes)\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:50.169 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=3\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:50.194 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:281 - Got item #3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4.80s] RESULT: job_000 - Red Hot Chili Peppers - O... (7833KB)\n",
      "[  4.80s] RESULT: job_001 - Drake - Drew A Picasso.mp... (7747KB)\n",
      "[  4.81s] SUBMIT: job_003 (Tom Misch - Isn't She Lov...) - 2638KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:50.679 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=4\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:50.898 | INFO     | __main__:mock_preprocess:48 - Processed Chris Brown, Drake - No Guidan... (8454211 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.73s] RESULT: job_002 - Chris Brown, Drake - No G... (8256KB)\n",
      "[  5.73s] RESULT: job_003 - Tom Misch - Isn't She Lov... (2638KB)\n",
      "[  5.83s] SUBMIT: job_004 (Matt Quentin - Morning De...) - 7129KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:51.100 | INFO     | __main__:mock_preprocess:48 - Processed Tom Misch - Isn't She Lovely.m... (2701416 bytes)\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:51.240 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:281 - Got item #4\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:51.751 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6.84s] SUBMIT: job_005 (Doja Cat - Streets.mp3...) - 8340KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:52.258 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=6\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:52.464 | INFO     | __main__:mock_preprocess:48 - Processed Matt Quentin - Morning Dew.mp3... (7300333 bytes)\n",
      "2025-12-31 03:53:52,577\tINFO progress_bar.py:215 -- ReadQueueStreaming->SplitBlocks(200): Tasks: 1; Actors: 0; Queued blocks: 0 (0.0B); Resources: 1.0 CPU, 28.8MiB object store: Progress Completed 6 / ?\n",
      "2025-12-31 03:53:52,578\tINFO progress_bar.py:215 -- MapBatches(AgentCallable): Tasks: 1; Actors: 2; Queued blocks: 0 (0.0B); Resources: 2.0 CPU, 120.0B object store; [all objects local]: Progress Completed 4 / ?\n",
      "2025-12-31 03:53:52,579\tINFO progress_bar.py:215 -- Running Dataset: dataset_1_0. Active & requested resources: 3/10 CPU, 28.8MiB/1.0GiB object store: Progress Completed 4 / ?\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:52.667 | INFO     | __main__:mock_preprocess:48 - Processed Doja Cat - Streets.mp3... (8540748 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  7.31s] RESULT: job_004 - Matt Quentin - Morning De... (7129KB)\n",
      "[  7.31s] RESULT: job_005 - Doja Cat - Streets.mp3... (8340KB)\n",
      "[  7.85s] SUBMIT: job_006 (mt. fujitive - sundown.mp...) - 2996KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:53.319 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.88s] SUBMIT: job_007 (NAV, Don Toliver - YOU (F...) - 5738KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:54.376 | DEBUG    | src.streaming_pipeline.streaming_datasource:make_block_generator:334 - Yielding batch with 1 rows, total_read=8\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:54.378 | INFO     | src.streaming_pipeline.streaming_datasource:make_block_generator:260 - Reached max_items limit (8)\n",
      "\u001b[36m(ReadQueueStreaming->SplitBlocks(200) pid=9353)\u001b[0m 2025-12-31 03:53:54.379 | INFO     | src.streaming_pipeline.streaming_datasource:make_block_generator:345 - StreamingDatasource reader exiting, read 8 items\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:54.589 | INFO     | __main__:mock_preprocess:48 - Processed mt. fujitive - sundown.mp3... (3068344 bytes)\n",
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9352)\u001b[0m 2025-12-31 03:53:49.575 | INFO     | src.streaming_pipeline.agent:__init__:301 - AgentCallable initialized: FunctionAgent\n",
      "\u001b[32m2025-12-31 03:53:54.796\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.streaming_pipeline.streaming_datasource\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m180\u001b[0m - \u001b[1mStreamingDatasource stop requested\u001b[0m\n",
      "2025-12-31 03:53:54,796\tINFO streaming_executor.py:304 -- ✔️  Dataset dataset_1_0 execution finished in 7.37 seconds\n",
      "\u001b[32m2025-12-31 03:53:54.797\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.streaming_pipeline.streaming_component\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m286\u001b[0m - \u001b[1mPipeline StreamingTest stopped\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.41s] RESULT: job_006 - mt. fujitive - sundown.mp... (2996KB)\n",
      "[  9.41s] RESULT: job_007 - NAV, Don Toliver - YOU (F... (5738KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(MapWorker(MapBatches(AgentCallable)) pid=9351)\u001b[0m 2025-12-31 03:53:54.791 | INFO     | __main__:mock_preprocess:48 - Processed NAV, Don Toliver - YOU (FT DON... (5875969 bytes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.88s] PRODUCER_DONE: All 8 jobs submitted\n",
      "\n",
      "============================================================\n",
      "STREAMING ANALYSIS\n",
      "============================================================\n",
      "\n",
      "Timing:\n",
      "  First result at:     4.80s\n",
      "  Producer done at:    9.88s\n",
      "  Last result at:      9.41s\n",
      "\n",
      "Streaming metrics:\n",
      "  Results before producer done: 8/8\n",
      "\n",
      "✅ STREAMING CONFIRMED!\n",
      "   First result arrived 5.08s BEFORE all jobs submitted\n",
      "\n",
      "------------------------------------------------------------\n",
      "TIMELINE:\n",
      "------------------------------------------------------------\n",
      "    [  1.75s] SUBMIT          job_000 (Red Hot Chili Peppers - O...) - 7833KB\n",
      "    [  2.77s] SUBMIT          job_001 (Drake - Drew A Picasso.mp...) - 7747KB\n",
      "    [  3.79s] SUBMIT          job_002 (Chris Brown, Drake - No G...) - 8256KB\n",
      ">>> [  4.80s] RESULT          job_000 - Red Hot Chili Peppers - O... (7833KB)\n",
      ">>> [  4.80s] RESULT          job_001 - Drake - Drew A Picasso.mp... (7747KB)\n",
      "    [  4.81s] SUBMIT          job_003 (Tom Misch - Isn't She Lov...) - 2638KB\n",
      ">>> [  5.73s] RESULT          job_002 - Chris Brown, Drake - No G... (8256KB)\n",
      ">>> [  5.73s] RESULT          job_003 - Tom Misch - Isn't She Lov... (2638KB)\n",
      "    [  5.83s] SUBMIT          job_004 (Matt Quentin - Morning De...) - 7129KB\n",
      "    [  6.84s] SUBMIT          job_005 (Doja Cat - Streets.mp3...) - 8340KB\n",
      ">>> [  7.31s] RESULT          job_004 - Matt Quentin - Morning De... (7129KB)\n",
      ">>> [  7.31s] RESULT          job_005 - Doja Cat - Streets.mp3... (8340KB)\n",
      "    [  7.85s] SUBMIT          job_006 (mt. fujitive - sundown.mp...) - 2996KB\n",
      "    [  8.88s] SUBMIT          job_007 (NAV, Don Toliver - YOU (F...) - 5738KB\n",
      ">>> [  9.41s] RESULT          job_006 - mt. fujitive - sundown.mp... (2996KB)\n",
      ">>> [  9.41s] RESULT          job_007 - NAV, Don Toliver - YOU (F... (5738KB)\n",
      "    [  9.88s] PRODUCER_DONE   All 8 jobs submitted\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import time\n",
    "import threading\n",
    "import sys\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from ray.util.queue import Queue\n",
    "\n",
    "ray.shutdown()\n",
    "time.sleep(1)\n",
    "\n",
    "PROJECT_ROOT = \"/Users/rajsingh/Desktop/code/music_audio_analyzer\"\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "ray.init(\n",
    "  runtime_env={\n",
    "      \"working_dir\": PROJECT_ROOT,\n",
    "      \"excludes\": [\"*.mp3\", \"*.wav\", \"audio_files/\", \".git/\", \"__pycache__/\"],\n",
    "      \"env_vars\": {\"_CACHE_BUST\": str(time.time())}\n",
    "  }\n",
    ")\n",
    "\n",
    "from src.streaming_pipeline import (\n",
    "  FunctionAgent,\n",
    "  AgentRayComputeConfig,\n",
    "  AgentStage,\n",
    "  QueueStreamingDatasource,\n",
    "  StreamingDatasourceConfig,\n",
    "  StreamingPipeline,\n",
    ")\n",
    "\n",
    "# Simulated \"preprocessing\" - just returns size info\n",
    "def mock_preprocess(items):\n",
    "  import time\n",
    "  from loguru import logger\n",
    "\n",
    "  results = []\n",
    "  for item in items:\n",
    "      # Simulate some processing time (100-300ms per item)\n",
    "      time.sleep(0.2)\n",
    "\n",
    "      audio_bytes = item.get(\"audio_bytes\", b\"\")\n",
    "      results.append({\n",
    "          \"job_id\": item[\"job_id\"],\n",
    "          \"filename\": item[\"filename\"],\n",
    "          \"audio_size\": len(audio_bytes),\n",
    "          \"processed\": True,\n",
    "      })\n",
    "      logger.info(f\"Processed {item['filename'][:30]}... ({len(audio_bytes)} bytes)\")\n",
    "\n",
    "  return results\n",
    "\n",
    "# Get audio files\n",
    "AUDIO_DIR = Path(PROJECT_ROOT) / \"audio_files\"\n",
    "AUDIO_FILES = list(AUDIO_DIR.glob(\"*.mp3\"))[:8]  # Use 8 files\n",
    "TOTAL_JOBS = len(AUDIO_FILES)\n",
    "print(f\"Using {TOTAL_JOBS} audio files\")\n",
    "\n",
    "# Create queue\n",
    "job_queue = Queue(maxsize=100)\n",
    "\n",
    "datasource = QueueStreamingDatasource(\n",
    "  queue=job_queue,\n",
    "  item_to_row_fn=lambda x: x,\n",
    "  config=StreamingDatasourceConfig(\n",
    "      batch_size=2,\n",
    "      batch_timeout=0.5,\n",
    "      max_items=TOTAL_JOBS,\n",
    "  ),\n",
    ")\n",
    "\n",
    "stage = AgentStage(\n",
    "  agent=FunctionAgent(process_fn=mock_preprocess),\n",
    "  config=AgentRayComputeConfig(num_actors=2, batch_size=2),  # 2 parallel workers\n",
    "  name=\"MockPreprocessor\",\n",
    ")\n",
    "\n",
    "pipeline = StreamingPipeline(datasource=datasource, stages=[stage], name=\"StreamingTest\")\n",
    "\n",
    "# Tracking\n",
    "events = []\n",
    "start_time = time.time()\n",
    "\n",
    "def log_event(event_type, details):\n",
    "  elapsed = time.time() - start_time\n",
    "  events.append((elapsed, event_type, details))\n",
    "  print(f\"[{elapsed:6.2f}s] {event_type}: {details}\")\n",
    "\n",
    "# Slow producer - submits jobs 1 second apart\n",
    "def slow_producer():\n",
    "  for i, audio_file in enumerate(AUDIO_FILES):\n",
    "      audio_bytes = audio_file.read_bytes()\n",
    "      job = {\n",
    "          \"job_id\": f\"job_{i:03d}\",\n",
    "          \"filename\": audio_file.name,\n",
    "          \"audio_bytes\": audio_bytes,\n",
    "      }\n",
    "      job_queue.put(job)\n",
    "      log_event(\"SUBMIT\", f\"job_{i:03d} ({audio_file.name[:25]}...) - {len(audio_bytes)//1024}KB\")\n",
    "      time.sleep(1.0)  # 1 second between submissions\n",
    "  log_event(\"PRODUCER_DONE\", f\"All {TOTAL_JOBS} jobs submitted\")\n",
    "\n",
    "# Start producer\n",
    "t = threading.Thread(target=slow_producer, daemon=True)\n",
    "t.start()\n",
    "\n",
    "# Consume results\n",
    "results = []\n",
    "try:\n",
    "  for batch in pipeline.stream(batch_size=1):\n",
    "      if batch:\n",
    "          keys = list(batch.keys())\n",
    "          n = len(batch[keys[0]]) if keys else 0\n",
    "          for i in range(n):\n",
    "              result = {k: batch[k][i] for k in keys}\n",
    "              results.append(result)\n",
    "              log_event(\"RESULT\", f\"{result['job_id']} - {result['filename'][:25]}... ({result['audio_size']//1024}KB)\")\n",
    "\n",
    "      if len(results) >= TOTAL_JOBS:\n",
    "          break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "  print(\"Interrupted\")\n",
    "except Exception as e:\n",
    "  print(f\"ERROR: {e}\")\n",
    "  import traceback\n",
    "  traceback.print_exc()\n",
    "finally:\n",
    "  pipeline.stop()\n",
    "  t.join(timeout=2)\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STREAMING ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "submit_events = [(t, d) for t, e, d in events if e == \"SUBMIT\"]\n",
    "result_events = [(t, d) for t, e, d in events if e == \"RESULT\"]\n",
    "producer_done = [t for t, e, d in events if e == \"PRODUCER_DONE\"]\n",
    "\n",
    "if result_events and producer_done:\n",
    "  first_result = result_events[0][0]\n",
    "  done_time = producer_done[0]\n",
    "  results_before_done = sum(1 for t, _ in result_events if t < done_time)\n",
    "\n",
    "  print(f\"\\nTiming:\")\n",
    "  print(f\"  First result at:     {first_result:.2f}s\")\n",
    "  print(f\"  Producer done at:    {done_time:.2f}s\")\n",
    "  print(f\"  Last result at:      {result_events[-1][0]:.2f}s\")\n",
    "\n",
    "  print(f\"\\nStreaming metrics:\")\n",
    "  print(f\"  Results before producer done: {results_before_done}/{len(result_events)}\")\n",
    "\n",
    "  if first_result < done_time:\n",
    "      print(f\"\\n✅ STREAMING CONFIRMED!\")\n",
    "      print(f\"   First result arrived {done_time - first_result:.2f}s BEFORE all jobs submitted\")\n",
    "  else:\n",
    "      print(f\"\\n❌ NOT STREAMING - results waited for all submissions\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"TIMELINE:\")\n",
    "print(\"-\" * 60)\n",
    "for elapsed, event_type, details in sorted(events):\n",
    "  marker = \">>>\" if event_type == \"RESULT\" else \"   \"\n",
    "  print(f\"{marker} [{elapsed:6.2f}s] {event_type:15s} {details[:50]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b70b16-ffae-471a-9f97-83e4f36e8cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
