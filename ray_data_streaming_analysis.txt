This is a great question. After analyzing both your codebase and the Ray Data source, I can explain the key concepts and what you need to do.

## The Core Problem

You have a **push-based queue model**:
```
Producer → JobQueue → Dispatcher → [Pool] → WaveformQueue → Detector → ResultQueue
```

Ray Data uses a **pull-based operator topology model**:
```
Datasource → Operator → Operator → OutputIterator
```

The challenge: Ray Data streams beautifully for *finite* datasets, but your queues are *continuously filling* from external producers.

---

## How Ray Data Streaming Actually Works

Looking at `streaming_executor.py`, the key insight is in the control loop:

1. **`OutputIterator`** runs on your main thread (the consumer)
2. **Control loop** runs on a daemon thread, calling `ray.wait()` on pending tasks
3. **Operators** are state machines that pull from predecessors and push to successors
4. **RefBundles** (block references) flow through the topology as soon as they're ready

The streaming happens because:
- `ReadTask.__call__()` is a **generator** that `yields` blocks (see `datasource.py:385-405`)
- Operators don't wait for all input before emitting output
- `ray.wait()` triggers scheduling as soon as *any* task completes

---

## The Critical Insight: `ReadTask` is a Generator

From `datasource.py:385`:
```python
def __call__(self) -> Iterable[Block]:
    result = self._read_fn()
    # ...
    for block in result:
        yield block  # STREAMING - yields as blocks become available
```

**This is where you need to hook in.** Your `QueueDatasource` needs to yield blocks forever (or until signaled to stop), not just drain a queue once.

---

## What Your Current QueueDatasource Does Wrong

Your `queue_datasource.py:15-38` batches items and returns them:
```python
def _read_queue(self) -> Iterable[Dict[str, Any]]:
    while True:
        batch = []
        for _ in range(self.batch_size):
            item = self.queue.get_nowait()
            # ...
        if batch:
            yield batch  # Yields once, then ends when queue is empty
```

This **terminates** when the queue is temporarily empty, which is wrong for continuous streaming.

---

## The Solution: Infinite Generator with Sentinel

You need your `ReadTask` to be an **infinite generator** that only terminates on an explicit signal:

```python
def _read_queue(self) -> Iterable[Block]:
    while True:
        try:
            item = self.queue.get(timeout=self.poll_interval)
            if item is STOP_SENTINEL:
                return  # Clean shutdown
            # Batch items into blocks...
            yield block
        except queue.Empty:
            # Queue temporarily empty - KEEP WAITING, don't exit
            continue
```

---

## How to Abstract Your Components

Looking at your pattern in `pipeline.py`, each component has:
1. **Input queue** (or datasource)
2. **Processing logic** (actor pool)
3. **Output queue** (or next operator)
4. **Config** (num_actors, batch_size, max_pending_tasks)

To make this work with Ray Data's model, you need to understand the **`PhysicalOperator`** interface from `execution/interfaces/physical_operator.py`:

| Your Pattern | Ray Data Equivalent |
|--------------|---------------------|
| Dispatcher pulls from queue | `Operator.get_next()` pulls RefBundle from predecessor |
| Actor pool processes | `Operator.add_input()` schedules tasks via ray.remote |
| Push to output queue | `Operator.get_next()` returns completed RefBundle |
| Backpressure via queue size | `ResourceManager` budgets CPU/GPU/memory |

---

## The Streaming Architecture You Need

Here's the conceptual model for truly streaming Ray Data:

```
┌─────────────────────────────────────────────────────────────────┐
│  StreamingDatasource (infinite generator)                       │
│  - Wraps your external producer                                 │
│  - Yields RefBundles as data arrives                           │
│  - Never terminates until STOP signal                          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  MapOperator (preprocessing)                                    │
│  - Ray Data's built-in map operator                            │
│  - Configured with compute=ray.data.ActorPoolStrategy(size=N)  │
│  - Automatic batching and backpressure                         │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  MapOperator (detection)                                        │
│  - ActorPoolStrategy with num_gpus=1                           │
│  - batch_size parameter for GPU efficiency                     │
└─────────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────────┐
│  OutputIterator                                                 │
│  - iter_batches() pulls results as they complete               │
│  - Fully streaming, no materialization                         │
└─────────────────────────────────────────────────────────────────┘
```

---

## The Key Ray Data Classes to Study

1. **`Datasource`** (`datasource/datasource.py:225`) - Your entry point. Override `get_read_tasks()` to return tasks that yield blocks infinitely.

2. **`ActorPoolStrategy`** (`execution/operators/actor_pool_map_operator.py`) - This is exactly your dispatcher pattern! It:
   - Manages a pool of actors
   - Distributes work with `_dispatch_tasks()`
   - Applies backpressure via `_actor_pool_budget`

3. **`StreamingExecutor`** (`streaming_executor.py:159`) - The orchestrator. Uses `ray.wait()` exactly like your dispatcher does.

4. **`OutputIterator`** (`execution/streaming_executor.py:91`) - The consumer interface. Blocks on `_output_queue.get()` waiting for RefBundles.

---

## Why Ray Data Examples Don't Stream "Out of the Box"

The examples you've seen likely use:
```python
ray.data.read_parquet("files/*.parquet")
```

This creates a **finite** datasource that terminates after reading all files. For continuous streaming, you need a custom datasource that:

1. **Never returns from `ReadTask.__call__()`** until explicitly stopped
2. **Yields blocks as data arrives** (not batches all at once)
3. **Handles backpressure** by slowing down when downstream is overwhelmed

---

## Summary: What To Do

1. **Create an abstract `StreamingComponent` base class** with:
   - `input_schema` / `output_schema`
   - `num_actors`, `batch_size`, `max_pending_tasks`
   - `process(batch) -> batch` method

2. **Implement a `StreamingDatasource`** that:
   - Takes your external queue/producer as input
   - Has `get_read_tasks()` return a single task with an infinite generator
   - Yields blocks as data arrives, never terminates early

3. **Use Ray Data's `map_batches()` with `ActorPoolStrategy`**:
   ```python
   ds.map_batches(
       preprocess_fn,
       compute=ray.data.ActorPoolStrategy(size=num_preprocessors),
       batch_size=batch_size
   )
   ```

4. **The pipeline object becomes just configuration**:
   ```python
   pipeline = Pipeline([
       Component(PreprocessorActor, num_actors=4, batch_size=32),
       Component(DetectorActor, num_actors=2, num_gpus=1, batch_size=16),
   ])
   ```

The magic is in the `StreamingDatasource` - that's where the continuous streaming happens. Everything downstream is handled by Ray Data's existing streaming executor.
